{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Implenetation of a Variational Autoencoder\r\n",
    "\r\n",
    "A simple implementation of a VAE on the following dataset:\r\n",
    "* https://www.kaggle.com/mloey1/covid19-chest-ct-image-augmentation-gan-dataset \r\n",
    "\r\n",
    "Purpose is to examine and compare results to the generated data from the following paper, where they used a CGAN to improve classification results:\r\n",
    "* https://link.springer.com/article/10.1007%2Fs00521-020-05437-x "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torchvision.transforms as transforms\r\n",
    "import torchvision.datasets as datasets\r\n",
    "from torchvision.utils import make_grid\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import IPython.display as display\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from tqdm import tqdm\r\n",
    "import copy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "425\n",
      "118\n",
      "199\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter settings and data loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "\r\n",
    "# hyperparameter settings #######################\r\n",
    "# latent = number of latent dimensions\r\n",
    "# nE = number of epochs\r\n",
    "# bs = batch size\r\n",
    "# cap = capacity of network\r\n",
    "# lr = learning rate of optimizer\r\n",
    "# wd = weight decay\r\n",
    "# vb = variational beta\r\n",
    "runs = [\r\n",
    "    # {\"latent\":2, \"nE\":100, \"bs\":128, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1} # batch 1\r\n",
    "    # ,{\"latent\":6, \"nE\":100, \"bs\":128, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1} # batch 1\r\n",
    "    # ,{\"latent\":10, \"nE\":100, \"bs\":128, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1} # batch 1\r\n",
    "    # ,{\"latent\":20, \"nE\":100, \"bs\":128, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1} # batch 1\r\n",
    "    # {\"latent\":20, \"nE\":100, \"bs\":64, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1}\r\n",
    "    # ,{\"latent\":20, \"nE\":200, \"bs\":64, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1}\r\n",
    "    # ,{\"latent\":20, \"nE\":100, \"bs\":64, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":0.5}\r\n",
    "    {\"latent\":200, \"nE\":1000, \"bs\":64, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":0.5}\r\n",
    "        ]\r\n",
    "\r\n",
    "train_dir='../input/covid19-chest-ct-image-augmentation-gan-dataset/COVID-19/COVID-19/train'\r\n",
    "\r\n",
    "train_dataset = datasets.ImageFolder(\r\n",
    "    train_dir,\r\n",
    "    transforms.Compose([transforms.Resize( (256,256)) ,\r\n",
    "        transforms.ToTensor(),\r\n",
    "    ]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def show_image(image_tensor, num_images=25, size=(1, 28, 28)):\r\n",
    "    '''\r\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\r\n",
    "    size per image, plots and prints the images in an uniform grid.\r\n",
    "    '''\r\n",
    "    image_tensor = (image_tensor + 1) / 2\r\n",
    "    image_unflat = image_tensor.detach().cpu()\r\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\r\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "def plot_loss(training_loss, settings):\r\n",
    "    fig, ax = plt.subplots()\r\n",
    "    ax.plot(range(len(training_loss)), training_loss)\r\n",
    "\r\n",
    "    ax.set(xlabel='epochs', ylabel='BCE loss')\r\n",
    "    ax.set_title(f'Loss for {settings}', y=1.1)\r\n",
    "    ax.grid()\r\n",
    "\r\n",
    "    fig.savefig(\"test.png\")\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up VAE architecture"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Encoder(nn.Module):\r\n",
    "    '''encoder for VAE, goes from image conv net to linear latent layer'''\r\n",
    "    def __init__(self, capacity, latent_dims):\r\n",
    "        super(Encoder, self).__init__()\r\n",
    "        c = capacity\r\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = c, kernel_size = 4, stride = 2, padding = 1)\r\n",
    "        self.conv2 = nn.Conv2d(in_channels = c, out_channels = c*2, kernel_size = 4, stride = 2, padding = 1)\r\n",
    "        self.conv3 = nn.Conv2d(in_channels = c*2, out_channels = c*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n",
    "        self.conv4 = nn.Conv2d(in_channels = c*2*2, out_channels = c*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n",
    "        self.conv5 = nn.Conv2d(in_channels = c*2*2*2, out_channels = c*2*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n",
    "        self.conv6 = nn.Conv2d(in_channels = c*2*2*2*2, out_channels = c*2*2*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n",
    "        self.fc_mu = nn.Linear(in_features = 32768, out_features = latent_dims)\r\n",
    "        self.fc_logvar = nn.Linear(in_features = 32768, out_features = latent_dims)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = F.relu(self.conv1(x))\r\n",
    "        x = F.relu(self.conv2(x))\r\n",
    "        x = F.relu(self.conv3(x))\r\n",
    "        x = F.relu(self.conv4(x))\r\n",
    "        x = F.relu(self.conv5(x))\r\n",
    "        x = F.relu(self.conv6(x))\r\n",
    "        x = x.view(x.size(0), -1) # flatten feature maps to feature vectors for linear layers\r\n",
    "        x_mu = self.fc_mu(x)\r\n",
    "        x_logvar = self.fc_logvar(x)\r\n",
    "        return x_mu, x_logvar\r\n",
    "\r\n",
    "class Decoder(nn.Module):\r\n",
    "    '''decoder for VAE, goes from linear latent layer to deconv layers to reconstruct image'''\r\n",
    "    def __init__(self, capacity, latent_dims):\r\n",
    "        super(Decoder, self).__init__()\r\n",
    "        c = capacity\r\n",
    "        self.fc = nn.Linear(in_features = latent_dims, out_features = 32768)\r\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels = c, out_channels = 3, kernel_size=4, stride=2, padding=1)\r\n",
    "        self.conv2 = nn.ConvTranspose2d(out_channels = c, in_channels = c*2, kernel_size = 4, stride = 2, padding = 1)\r\n",
    "        self.conv3 = nn.ConvTranspose2d(out_channels = c*2, in_channels = c*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n",
    "        self.conv4 = nn.ConvTranspose2d(out_channels = c*2*2, in_channels = c*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n",
    "        self.conv5 = nn.ConvTranspose2d(out_channels = c*2*2*2, in_channels = c*2*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n",
    "        self.conv6 = nn.ConvTranspose2d(out_channels = c*2*2*2*2, in_channels = c*2*2*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.fc(x)\r\n",
    "        x = x.view(x.size(0), 2048, 4, 4) # unflatten feature vectors to feature maps for conv layers\r\n",
    "        x = F.relu(self.conv6(x))\r\n",
    "        x = F.relu(self.conv5(x))\r\n",
    "        x = F.relu(self.conv4(x))\r\n",
    "        x = F.relu(self.conv3(x))\r\n",
    "        x = F.relu(self.conv2(x))\r\n",
    "        x = torch.sigmoid(self.conv1(x)) # using BCE (Binary Crossentropy) as reconstruction loss, so output is sigmoid\r\n",
    "        return x\r\n",
    "\r\n",
    "class VAE(nn.Module):\r\n",
    "    '''VAE architecture for encoder -> sample from latent -> decode latent sample'''\r\n",
    "    def __init__(self, capacity, latent_dims):\r\n",
    "        super(VAE, self).__init__()\r\n",
    "        self.encoder = Encoder(capacity, latent_dims)\r\n",
    "        self.decoder = Decoder(capacity, latent_dims)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        latent_mu, latent_logvar = self.encoder(x)\r\n",
    "        latent = self.latent_sample(latent_mu, latent_logvar) # sample an image from latent distribution\r\n",
    "        x_recon = self.decoder(latent)\r\n",
    "        return x_recon, latent_mu, latent_logvar\r\n",
    "\r\n",
    "    def latent_sample(self, mu, logvar):\r\n",
    "        if self.training:\r\n",
    "            # the reparamterization trick\r\n",
    "            std = logvar.mul(0.5).exp_()\r\n",
    "            eps = torch.empty_like(std).normal_()\r\n",
    "            return eps.mul(std).add_(mu)\r\n",
    "        else:\r\n",
    "            return mu\r\n",
    "\r\n",
    "def vae_loss(recon_x, x, mu, logvar, variational_beta): # pass variational beta\r\n",
    "    recon_loss = F.binary_cross_entropy(recon_x.view(-1, 65536), x.view(-1, 65536), reduction = 'sum')\r\n",
    "\r\n",
    "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\r\n",
    "\r\n",
    "    return recon_loss + variational_beta * kldivergence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## setting up functions for training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def setup_model(capacity, latent_dims):\r\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "    print(device)\r\n",
    "\r\n",
    "    vae = VAE(capacity = capacity, latent_dims = latent_dims).to(device)\r\n",
    "    print(vae)\r\n",
    "\r\n",
    "    return vae"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(vae, train_loader, n_epochs, learning_rate, weight_decay, variational_beta):\r\n",
    "\r\n",
    "    optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=weight_decay)\r\n",
    "\r\n",
    "\r\n",
    "    # set to training mode\r\n",
    "    vae.train()\r\n",
    "\r\n",
    "    train_loss = []\r\n",
    "    best_model_wts = None\r\n",
    "    bmw_epoch = 0\r\n",
    "\r\n",
    "\r\n",
    "    print('Training ...')\r\n",
    "    for epoch in tqdm(range(n_epochs)):\r\n",
    "        \r\n",
    "        num_batches = 0\r\n",
    "        avg_loss = 0\r\n",
    "        best_loss = 0\r\n",
    "        \r\n",
    "        image_batch_recon = None\r\n",
    "        \r\n",
    "        for image_batch, _ in train_loader:\r\n",
    "            \r\n",
    "            image_batch = image_batch.to(device)\r\n",
    "\r\n",
    "            # vae reconstruction\r\n",
    "            image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\r\n",
    "            \r\n",
    "            # reconstruction error\r\n",
    "            loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar, variational_beta)\r\n",
    "            \r\n",
    "            # backpropagation\r\n",
    "            optimizer.zero_grad()\r\n",
    "            loss.backward()\r\n",
    "            \r\n",
    "            # one step of the optmizer (using the gradients from backpropagation)\r\n",
    "            optimizer.step()\r\n",
    "\r\n",
    "            avg_loss += loss.item()\r\n",
    "            num_batches += 1\r\n",
    "            \r\n",
    "        if (epoch % (n_epochs//10) == 0 and epoch != 0) or epoch == n_epochs-1:\r\n",
    "            show_image(image_batch_recon)\r\n",
    "        avg_loss /= num_batches\r\n",
    "        train_loss.append(avg_loss)\r\n",
    "\r\n",
    "        ## best weight\r\n",
    "        if epoch == 0:\r\n",
    "            best_loss = avg_loss\r\n",
    "            best_model_wts = copy.deepcopy(vae.state_dict())\r\n",
    "        if avg_loss < best_loss:\r\n",
    "            best_model_wts = copy.deepcopy(vae.state_dict())\r\n",
    "            best_loss = avg_loss\r\n",
    "            bmw_epoch = epoch + 1\r\n",
    "    \r\n",
    "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, n_epochs, sum(train_loss)/len(train_loss)))\r\n",
    "    return vae, best_model_wts, bmw_epoch, train_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for k, settings in enumerate(runs):\r\n",
    "\r\n",
    "    print(f\"starting run ... {k}/{len(runs)}\")\r\n",
    "    print(settings)\r\n",
    "\r\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=settings[\"bs\"], shuffle=True, num_workers=0)\r\n",
    "\r\n",
    "    vae = setup_model(settings[\"cap\"], settings[\"latent\"])\r\n",
    "\r\n",
    "    vae, best_model_wts, bmw_epoch, train_loss = train(vae, train_loader, settings[\"nE\"], settings[\"lr\"], settings[\"wd\"], settings[\"vb\"])\r\n",
    "    last_modeL_wts = vae.state_dict()\r\n",
    "\r\n",
    "    plot_loss(train_loss, settings)\r\n",
    "\r\n",
    "    print(\"-------------------------------------------------------------------\")\r\n",
    "    print(\"saving weights for model\")\r\n",
    "\r\n",
    "    torch.save(vae.state_dict(), f'n_{settings[\"nE\"]}.ld_{settings[\"latent\"]}.lr_{settings[\"lr\"]}.vb_{settings[\"vb\"]}.last_model.wts')\r\n",
    "    vae.load_state_dict(best_model_wts)\r\n",
    "    torch.save(vae.state_dict(), f'n_{bmw_epoch}.ld_{settings[\"latent\"]}.lr_{settings[\"lr\"]}.vb_{settings[\"vb\"]}.lowest_loss_model.wts')\r\n",
    "\r\n",
    "    print(\"-------------------------------------------------------------------\")\r\n",
    "    print()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "interpreter": {
   "hash": "0de36b31320ba4c88b4f85a74724f3d16c36a44df48581253710b1065e752d9e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}